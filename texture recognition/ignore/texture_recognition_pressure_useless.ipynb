{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Texture Recognition - Pressure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "import copy\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import lightly\n",
    "from lightly.models import utils\n",
    "from lightly.models.modules import heads\n",
    "from lightly.loss import NTXentLoss\n",
    "\n",
    "from lightly.models.modules.heads import SimSiamPredictionHead\n",
    "from lightly.models.modules.heads import SimSiamProjectionHead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GifDataset(Dataset):\n",
    "    def __init__(self, gif_dir, transform=None):\n",
    "        self.gif_dir = gif_dir\n",
    "        self.transform = transform\n",
    "        self.gif_files = [os.path.join(gif_dir, f) for f in os.listdir(gif_dir) if f.endswith('.gif')]\n",
    "        self.frames = self._extract_frames()\n",
    "\n",
    "    def _extract_frames(self):\n",
    "        frames = []\n",
    "        total_files = len(self.gif_files)\n",
    "        for i, gif_file in enumerate(self.gif_files):\n",
    "            gif = Image.open(gif_file)\n",
    "            total_frames = gif.n_frames\n",
    "            for frame in range(total_frames):\n",
    "                gif.seek(frame)\n",
    "                frame_image = gif.convert('RGB')\n",
    "                if self.transform:\n",
    "                    frame_image = self.transform(frame_image)\n",
    "                frames.append(frame_image)\n",
    "            # Calculate and print the progress\n",
    "            percent_complete = ((i + 1) / total_files) * 100\n",
    "            print(f'Progress: {percent_complete:.2f}%')\n",
    "        return frames\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.frames[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 0.38%\n",
      "Progress: 0.77%\n",
      "Progress: 1.15%\n",
      "Progress: 1.54%\n",
      "Progress: 1.92%\n",
      "Progress: 2.31%\n",
      "Progress: 2.69%\n",
      "Progress: 3.08%\n",
      "Progress: 3.46%\n",
      "Progress: 3.85%\n",
      "Progress: 4.23%\n",
      "Progress: 4.62%\n",
      "Progress: 5.00%\n",
      "Progress: 5.38%\n",
      "Progress: 5.77%\n",
      "Progress: 6.15%\n",
      "Progress: 6.54%\n",
      "Progress: 6.92%\n",
      "Progress: 7.31%\n",
      "Progress: 7.69%\n",
      "Progress: 8.08%\n",
      "Progress: 8.46%\n",
      "Progress: 8.85%\n",
      "Progress: 9.23%\n",
      "Progress: 9.62%\n",
      "Progress: 10.00%\n",
      "Progress: 10.38%\n",
      "Progress: 10.77%\n",
      "Progress: 11.15%\n",
      "Progress: 11.54%\n",
      "Progress: 11.92%\n",
      "Progress: 12.31%\n",
      "Progress: 12.69%\n",
      "Progress: 13.08%\n",
      "Progress: 13.46%\n",
      "Progress: 13.85%\n",
      "Progress: 14.23%\n",
      "Progress: 14.62%\n",
      "Progress: 15.00%\n",
      "Progress: 15.38%\n",
      "Progress: 15.77%\n",
      "Progress: 16.15%\n",
      "Progress: 16.54%\n",
      "Progress: 16.92%\n",
      "Progress: 17.31%\n",
      "Progress: 17.69%\n",
      "Progress: 18.08%\n",
      "Progress: 18.46%\n",
      "Progress: 18.85%\n",
      "Progress: 19.23%\n",
      "Progress: 19.62%\n",
      "Progress: 20.00%\n",
      "Progress: 20.38%\n",
      "Progress: 20.77%\n",
      "Progress: 21.15%\n",
      "Progress: 21.54%\n",
      "Progress: 21.92%\n",
      "Progress: 22.31%\n",
      "Progress: 22.69%\n",
      "Progress: 23.08%\n",
      "Progress: 23.46%\n",
      "Progress: 23.85%\n",
      "Progress: 24.23%\n",
      "Progress: 24.62%\n",
      "Progress: 25.00%\n",
      "Progress: 25.38%\n",
      "Progress: 25.77%\n",
      "Progress: 26.15%\n",
      "Progress: 26.54%\n",
      "Progress: 26.92%\n",
      "Progress: 27.31%\n",
      "Progress: 27.69%\n",
      "Progress: 28.08%\n",
      "Progress: 28.46%\n",
      "Progress: 28.85%\n",
      "Progress: 29.23%\n",
      "Progress: 29.62%\n",
      "Progress: 30.00%\n",
      "Progress: 30.38%\n",
      "Progress: 30.77%\n",
      "Progress: 31.15%\n",
      "Progress: 31.54%\n",
      "Progress: 31.92%\n",
      "Progress: 32.31%\n",
      "Progress: 32.69%\n",
      "Progress: 33.08%\n",
      "Progress: 33.46%\n",
      "Progress: 33.85%\n",
      "Progress: 34.23%\n",
      "Progress: 34.62%\n",
      "Progress: 35.00%\n",
      "Progress: 35.38%\n",
      "Progress: 35.77%\n",
      "Progress: 36.15%\n",
      "Progress: 36.54%\n",
      "Progress: 36.92%\n",
      "Progress: 37.31%\n",
      "Progress: 37.69%\n",
      "Progress: 38.08%\n",
      "Progress: 38.46%\n",
      "Progress: 38.85%\n",
      "Progress: 39.23%\n",
      "Progress: 39.62%\n",
      "Progress: 40.00%\n",
      "Progress: 40.38%\n",
      "Progress: 40.77%\n",
      "Progress: 41.15%\n",
      "Progress: 41.54%\n",
      "Progress: 41.92%\n",
      "Progress: 42.31%\n",
      "Progress: 42.69%\n",
      "Progress: 43.08%\n",
      "Progress: 43.46%\n",
      "Progress: 43.85%\n",
      "Progress: 44.23%\n",
      "Progress: 44.62%\n",
      "Progress: 45.00%\n",
      "Progress: 45.38%\n",
      "Progress: 45.77%\n",
      "Progress: 46.15%\n",
      "Progress: 46.54%\n",
      "Progress: 46.92%\n",
      "Progress: 47.31%\n",
      "Progress: 47.69%\n",
      "Progress: 48.08%\n",
      "Progress: 48.46%\n",
      "Progress: 48.85%\n",
      "Progress: 49.23%\n",
      "Progress: 49.62%\n",
      "Progress: 50.00%\n",
      "Progress: 50.38%\n",
      "Progress: 50.77%\n",
      "Progress: 51.15%\n",
      "Progress: 51.54%\n",
      "Progress: 51.92%\n",
      "Progress: 52.31%\n",
      "Progress: 52.69%\n",
      "Progress: 53.08%\n",
      "Progress: 53.46%\n",
      "Progress: 53.85%\n",
      "Progress: 54.23%\n",
      "Progress: 54.62%\n",
      "Progress: 55.00%\n",
      "Progress: 55.38%\n",
      "Progress: 55.77%\n",
      "Progress: 56.15%\n",
      "Progress: 56.54%\n",
      "Progress: 56.92%\n",
      "Progress: 57.31%\n",
      "Progress: 57.69%\n",
      "Progress: 58.08%\n",
      "Progress: 58.46%\n",
      "Progress: 58.85%\n",
      "Progress: 59.23%\n",
      "Progress: 59.62%\n",
      "Progress: 60.00%\n",
      "Progress: 60.38%\n",
      "Progress: 60.77%\n",
      "Progress: 61.15%\n",
      "Progress: 61.54%\n",
      "Progress: 61.92%\n",
      "Progress: 62.31%\n",
      "Progress: 62.69%\n",
      "Progress: 63.08%\n",
      "Progress: 63.46%\n",
      "Progress: 63.85%\n",
      "Progress: 64.23%\n",
      "Progress: 64.62%\n",
      "Progress: 65.00%\n",
      "Progress: 65.38%\n",
      "Progress: 65.77%\n",
      "Progress: 66.15%\n",
      "Progress: 66.54%\n",
      "Progress: 66.92%\n",
      "Progress: 67.31%\n",
      "Progress: 67.69%\n",
      "Progress: 68.08%\n",
      "Progress: 68.46%\n",
      "Progress: 68.85%\n",
      "Progress: 69.23%\n",
      "Progress: 69.62%\n",
      "Progress: 70.00%\n",
      "Progress: 70.38%\n",
      "Progress: 70.77%\n",
      "Progress: 71.15%\n",
      "Progress: 71.54%\n",
      "Progress: 71.92%\n",
      "Progress: 72.31%\n",
      "Progress: 72.69%\n",
      "Progress: 73.08%\n",
      "Progress: 73.46%\n",
      "Progress: 73.85%\n",
      "Progress: 74.23%\n",
      "Progress: 74.62%\n",
      "Progress: 75.00%\n",
      "Progress: 75.38%\n",
      "Progress: 75.77%\n",
      "Progress: 76.15%\n",
      "Progress: 76.54%\n",
      "Progress: 76.92%\n",
      "Progress: 77.31%\n",
      "Progress: 77.69%\n",
      "Progress: 78.08%\n",
      "Progress: 78.46%\n",
      "Progress: 78.85%\n",
      "Progress: 79.23%\n",
      "Progress: 79.62%\n",
      "Progress: 80.00%\n",
      "Progress: 80.38%\n",
      "Progress: 80.77%\n",
      "Progress: 81.15%\n",
      "Progress: 81.54%\n",
      "Progress: 81.92%\n",
      "Progress: 82.31%\n",
      "Progress: 82.69%\n",
      "Progress: 83.08%\n",
      "Progress: 83.46%\n",
      "Progress: 83.85%\n",
      "Progress: 84.23%\n",
      "Progress: 84.62%\n",
      "Progress: 85.00%\n",
      "Progress: 85.38%\n",
      "Progress: 85.77%\n",
      "Progress: 86.15%\n",
      "Progress: 86.54%\n",
      "Progress: 86.92%\n",
      "Progress: 87.31%\n",
      "Progress: 87.69%\n",
      "Progress: 88.08%\n",
      "Progress: 88.46%\n",
      "Progress: 88.85%\n",
      "Progress: 89.23%\n",
      "Progress: 89.62%\n",
      "Progress: 90.00%\n",
      "Progress: 90.38%\n",
      "Progress: 90.77%\n",
      "Progress: 91.15%\n",
      "Progress: 91.54%\n",
      "Progress: 91.92%\n",
      "Progress: 92.31%\n",
      "Progress: 92.69%\n",
      "Progress: 93.08%\n",
      "Progress: 93.46%\n",
      "Progress: 93.85%\n",
      "Progress: 94.23%\n",
      "Progress: 94.62%\n",
      "Progress: 95.00%\n",
      "Progress: 95.38%\n",
      "Progress: 95.77%\n",
      "Progress: 96.15%\n",
      "Progress: 96.54%\n",
      "Progress: 96.92%\n",
      "Progress: 97.31%\n",
      "Progress: 97.69%\n",
      "Progress: 98.08%\n",
      "Progress: 98.46%\n",
      "Progress: 98.85%\n",
      "Progress: 99.23%\n",
      "Progress: 99.62%\n",
      "Progress: 100.00%\n"
     ]
    }
   ],
   "source": [
    "gif_dataset = GifDataset(\"/mnt/g/textures/texture recognition/sample testing\", transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightly_dataset = lightly.data.LightlyDataset.from_torch_dataset(gif_dataset)\n",
    "gif_dataloader = torch.utils.data.DataLoader(\n",
    "    lightly_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ftrs = 512\n",
    "# dimension of the output of the prediction and projection heads\n",
    "out_dim = proj_hidden_dim = 512\n",
    "# the prediction head uses a bottleneck architecture\n",
    "pred_hidden_dim = 128\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimSiam(nn.Module):\n",
    "    def __init__(\n",
    "        self, backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.projection_head = SimSiamProjectionHead(\n",
    "            num_ftrs, proj_hidden_dim, out_dim\n",
    "        )\n",
    "        self.prediction_head = SimSiamPredictionHead(\n",
    "            out_dim, pred_hidden_dim, out_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # get representations\n",
    "        f = self.backbone(x).flatten(start_dim=1)\n",
    "        # get projections\n",
    "        z = self.projection_head(f)\n",
    "        # get predictions\n",
    "        p = self.prediction_head(z)\n",
    "        # stop gradient\n",
    "        z = z.detach()\n",
    "        return z, p\n",
    "\n",
    "\n",
    "# we use a pretrained resnet for this tutorial to speed\n",
    "# up training time but you can also train one from scratch\n",
    "resnet = torchvision.models.resnet18()\n",
    "backbone = nn.Sequential(*list(resnet.children())[:-1])\n",
    "model = SimSiam(backbone, num_ftrs, proj_hidden_dim, pred_hidden_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimSiam uses a symmetric negative cosine similarity loss\n",
    "criterion = lightly.loss.NegativeCosineSimilarity()\n",
    "\n",
    "# scale the learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# use SGD with momentum and weight decay\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=5e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 8\n",
    "# batch_size = 128\n",
    "batch_size = 512\n",
    "seed = 1\n",
    "# epochs = 50\n",
    "epochs = 800\n",
    "# input_size = 256\n",
    "input_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m avg_output_std \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m x0, x1, _, _ \u001b[38;5;129;01min\u001b[39;00m gif_dataloader:\n\u001b[1;32m     13\u001b[0m         \n\u001b[1;32m     14\u001b[0m         \u001b[38;5;66;03m# move images to the gpu\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         x0 \u001b[38;5;241m=\u001b[39m x0\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m         x1 \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.9/site-packages/lightly/data/dataset.py:231\u001b[0m, in \u001b[0;36mLightlyDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns (sample, target, fname) of item at index.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m fname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_to_filename(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, index)\n\u001b[0;32m--> 231\u001b[0m sample, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(index)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sample, target, fname\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "gpu_id = 3\n",
    "\n",
    "device = 'cuda:' + str(gpu_id) if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "avg_loss = 0.\n",
    "avg_output_std = 0.\n",
    "for e in range(epochs):\n",
    "\n",
    "    for x0, x1, _, _ in gif_dataloader:\n",
    "        \n",
    "        # move images to the gpu\n",
    "        x0 = x0.to(device)\n",
    "        x1 = x1.to(device)\n",
    "\n",
    "        # run the model on both transforms of the images\n",
    "        # we get projections (z0 and z1) and\n",
    "        # predictions (p0 and p1) as output\n",
    "        z0, p0 = model(x0)\n",
    "        z1, p1 = model(x1)\n",
    "\n",
    "        # apply the symmetric negative cosine similarity\n",
    "        # and run backpropagation\n",
    "        loss = 0.5 * (criterion(z0, p1) + criterion(z1, p0))\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # calculate the per-dimension standard deviation of the outputs\n",
    "        # we can use this later to check whether the embeddings are collapsing\n",
    "        output = p0.detach()\n",
    "        output = torch.nn.functional.normalize(output, dim=1)\n",
    "\n",
    "        output_std = torch.std(output, 0)\n",
    "        output_std = output_std.mean()\n",
    "\n",
    "        # use moving averages to track the loss and standard deviation\n",
    "        w = 0.9\n",
    "        avg_loss = w * avg_loss + (1 - w) * loss.item()\n",
    "        avg_output_std = w * avg_output_std + (1 - w) * output_std.item()\n",
    "\n",
    "    # the level of collapse is large if the standard deviation of the l2\n",
    "    # normalized output is much smaller than 1 / sqrt(dim)\n",
    "    collapse_level = max(0., 1 - math.sqrt(out_dim) * avg_output_std)\n",
    "    # print intermediate results\n",
    "    print(f'[Epoch {e:3d}] '\n",
    "        f'Loss = {avg_loss:.2f} | '\n",
    "        f'Collapse Level: {collapse_level:.2f} / 1.00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
